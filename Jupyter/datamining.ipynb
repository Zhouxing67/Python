{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[frozenset({1}), frozenset({9}), frozenset({8}), frozenset({7}), frozenset({6}), frozenset({5}), frozenset({4}), frozenset({3}), frozenset({2})], [frozenset({1, 2}), frozenset({6, 7})], []]\n",
      "frozenset({2}) ---> frozenset({1}) conf: 0.78125\n",
      "frozenset({1}) ---> frozenset({2}) conf: 0.78125\n",
      "frozenset({7}) ---> frozenset({6}) conf: 0.8928571428571428\n",
      "frozenset({6}) ---> frozenset({7}) conf: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "#生成Apriori算法的数据集\n",
    "def loadDataSet(numTransactions = 50, numItems = 10 , maxItemsPerTransaction = 10):\n",
    "    \"\"\"\n",
    "    生成Apriori算法的数据集\n",
    "\n",
    "    参数:\n",
    "    - numTransactions: 生成的事务数量\n",
    "    - numItems: 项的数量\n",
    "    - maxItemsPerTransaction: 每个事务中的最大项数量\n",
    "\n",
    "    返回值:\n",
    "    一个包含多个事务的列表，每个事务由多个项组成\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    dataset = []\n",
    "    for _ in range(numTransactions):\n",
    "        numItemsInTransaction = random.randint(1, maxItemsPerTransaction)\n",
    "        transaction = set(random.sample(range(1, numItems + 1), numItemsInTransaction))\n",
    "        dataset.append(transaction)\n",
    "\n",
    "    return dataset\n",
    "#根据数据库生成所有的一项集\n",
    "def createC1(dataSet):\n",
    "    C1=[]\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction:\n",
    "            if not [item] in C1:\n",
    "                C1.append([item])\n",
    "    C1.sort()\n",
    "    return list(map(frozenset,C1))\n",
    "#筛选候选集生成频繁项集   D: 事务数据库， Ck: 候选集， minsupport: 最小支持度\n",
    "def scanD(D, CK, minSupport):\n",
    "    ssCnt = {}\n",
    "    for tid in D:\n",
    "        for can in CK:\n",
    "            if can.issubset(tid):\n",
    "                if not can in ssCnt:\n",
    "                    ssCnt[can]=1\n",
    "                else:\n",
    "                    ssCnt[can]+=1\n",
    "\n",
    "    numItems = float(len(D))\n",
    "    retList = []\n",
    "    supportData={}\n",
    "    for key in ssCnt:\n",
    "        support = ssCnt[key]/numItems\n",
    "        if support>=minSupport:\n",
    "            retList.insert(0,key)\n",
    "            supportData[key]=support\n",
    "\n",
    "    return retList,supportData\n",
    "#频繁项集两两组合，生成新的候选项集\n",
    "def aprioriGen(Lk,k):\n",
    "    retList=[]\n",
    "    lenLk = len(Lk)\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i+1,lenLk):\n",
    "            L1=list(Lk[i])[:k-2]\n",
    "            L2=list(Lk[j])[:k-2]\n",
    "            L1.sort()\n",
    "            L2.sort()\n",
    "            if L1==L2:\n",
    "                retList.append(Lk[i] | Lk[j])\n",
    "    return retList\n",
    "#apriori算法实现\n",
    "def apriori(dataSet, minSupport=0.5):\n",
    "    C1 = createC1(dataSet)\n",
    "    D=list(map(set,dataSet))\n",
    "    L1,supportData = scanD(D, C1, minSupport)\n",
    "    L=[L1]\n",
    "    k=2\n",
    "    while(len(L[k-2])>0):\n",
    "        CK = aprioriGen(L[k-2], k)\n",
    "        Lk,supK = scanD(D, CK, minSupport)\n",
    "        supportData.update(supK)\n",
    "        L.append(Lk)\n",
    "        k += 1\n",
    "    return L, supportData\n",
    "#规则生成\n",
    "def generateRules(L, supportData, minConf=0.7):\n",
    "    \"\"\"\n",
    "    生成关联规则的函数\n",
    "\n",
    "    参数:\n",
    "        L: 频繁项集列表\n",
    "        supportData: 支持度字典\n",
    "        minConf: 最小置信度,默认为0.7\n",
    "\n",
    "    返回值:\n",
    "        bigRuleList: 存储所有满足最小置信度要求的关联规则的列表\n",
    "    \"\"\"\n",
    "\n",
    "    bigRuleList = []  # 存储所有的关联规则\n",
    "    for i in range(1, len(L)):\n",
    "        for freqSet in L[i]:\n",
    "            H1 = [frozenset([item]) for item in freqSet]  # 将freqSet划分为单个元素的集合列表H1\n",
    "            if (i > 1):\n",
    "                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)  # 生成关联规则\n",
    "            else:\n",
    "                calcConf(freqSet, H1, supportData, bigRuleList, minConf)  # 计算并存储关联规则\n",
    "    return bigRuleList\n",
    "#计算满足最小置信度要求的关联规则\n",
    "def calcConf(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    \"\"\"\n",
    "    计算满足最小置信度要求的关联规则的函数\n",
    "\n",
    "    参数:\n",
    "        freqSet: 频繁项集\n",
    "        H: 可能的后件集合\n",
    "        supportData: 支持度字典\n",
    "        brl: 存储规则的列表\n",
    "        minConf: 最小置信度,默认为0.7\n",
    "\n",
    "    返回值:\n",
    "        prunedH: 符合要求的后件列表\n",
    "    \"\"\"\n",
    "\n",
    "    prunedH = []  # 存储符合要求的后件\n",
    "    for conseq in H:\n",
    "        conf = supportData[freqSet] / supportData[freqSet - conseq]  # 计算置信度\n",
    "        if conf >= minConf:  # 满足最小置信度要求\n",
    "            print(freqSet - conseq, '--->', conseq, 'conf:', conf)  # 打印关联规则\n",
    "            brl.append((freqSet - conseq, conseq, conf))  # 将关联规则存储在brl中\n",
    "            prunedH.append(conseq)  # 将后件添加到prunedH中\n",
    "    return prunedH\n",
    "#从后件集合生成规则\n",
    "def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n",
    "    \"\"\"\n",
    "    从后件集合生成规则的函数\n",
    "\n",
    "    参数:\n",
    "        freqSet: 频繁项集\n",
    "        H: 可能的后件集合\n",
    "        supportData: 支持度字典\n",
    "        brl: 存储规则的列表\n",
    "        minConf: 最小置信度,默认为0.7\n",
    "\n",
    "    返回值:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    m = len(H[0])  # 后件长度\n",
    "    if len(freqSet) > (m + 1):  # 如果频繁项集的长度大于后件长度加一\n",
    "        Hmp1 = aprioriGen(H, m + 1)  # 生成下一层的后件集合\n",
    "        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)  # 计算满足最小置信度要求的关联规则\n",
    "        if len(Hmp1) > 1:  # 如果满足最小置信度要求的关联规则个数大于1\n",
    "            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)  # 递归调用自身，继续生成更多规则\n",
    "\n",
    "\n",
    "dataSet=loadDataSet()\n",
    "#print(\"Dataset:\")\n",
    "#for transaction in dataSet:\n",
    "#    print(transaction)\n",
    "L,supportData=apriori(dataSet)\n",
    "print(L)\n",
    "rules = generateRules(L,supportData,minConf=0.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (6, 1, 2)\n",
      "centroids : (2, 2)\n",
      "样本标签: [1 1 1 0 0 0]\n",
      "质心位置: [[4. 2.]\n",
      " [1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kmeans(X, k, max_iters=100):\n",
    "    # 随机初始化质心\n",
    "    centroids = np.random.choice(len(X), size=k, replace=False)\n",
    "    centroids = X[centroids]\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # 分配每个样本到最近的质心\n",
    "        labels = np.argmin(np.linalg.norm(X[:, np.newaxis] - centroids, axis=-1), axis=-1)\n",
    "        \n",
    "        # 更新质心位置\n",
    "        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n",
    "\n",
    "        # 判断是否收敛\n",
    "        if np.all(new_centroids == centroids):\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "    print(f'X shape : {X[:, np.newaxis].shape}')\n",
    "    print(f'centroids : {centroids.shape}')\n",
    "    return labels, centroids\n",
    "\n",
    "# 示例数据\n",
    "X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "# 调用K均值算法\n",
    "labels, centroids = kmeans(X, k=2)\n",
    "\n",
    "# 打印结果\n",
    "print(\"样本标签:\", labels)\n",
    "print(\"质心位置:\", centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 轮迭代结果：\n",
      "数据点到质心的距离：\n",
      "[[2.         3.16227766]\n",
      " [0.         5.09901951]\n",
      " [5.         3.60555128]\n",
      " [1.41421356 6.        ]\n",
      " [2.         7.07106781]\n",
      " [5.09901951 5.65685425]\n",
      " [1.41421356 4.        ]\n",
      " [5.09901951 0.        ]\n",
      " [4.47213595 4.24264069]\n",
      " [2.23606798 4.12310563]]\n",
      "簇质心的计算过程：\n",
      "[[4.28571429 5.71428571]\n",
      " [6.         2.66666667]]\n",
      "数据点所属的簇：\n",
      "[0 0 1 0 0 0 0 1 1 0]\n",
      "----------------------------------------\n",
      "第 2 轮迭代结果：\n",
      "数据点到质心的距离：\n",
      "[[2.14285714 3.2829526 ]\n",
      " [1.31707778 4.48454135]\n",
      " [3.83857967 1.05409255]\n",
      " [1.31707778 4.77260702]\n",
      " [2.62250854 6.11918658]\n",
      " [3.78234351 3.07318149]\n",
      " [0.76930926 3.07318149]\n",
      " [4.72293579 2.60341656]\n",
      " [3.21031501 1.66666667]\n",
      " [1.01015254 2.53859104]]\n",
      "簇质心的计算过程：\n",
      "[[3.66666667 5.83333333]\n",
      " [6.5        3.25      ]]\n",
      "数据点所属的簇：\n",
      "[0 0 1 0 0 1 0 1 1 0]\n",
      "----------------------------------------\n",
      "第 3 轮迭代结果：\n",
      "数据点到质心的距离：\n",
      "[[1.95078332 3.57945527]\n",
      " [0.68718427 4.45112345]\n",
      " [4.37480158 0.55901699]\n",
      " [1.21335165 4.50693909]\n",
      " [2.26691175 5.90021186]\n",
      " [4.4127341  2.30488611]\n",
      " [0.89752747 3.0516389 ]\n",
      " [4.84481395 3.36340601]\n",
      " [3.8042374  0.90138782]\n",
      " [1.57233019 2.30488611]]\n",
      "簇质心的计算过程：\n",
      "[[3.66666667 5.83333333]\n",
      " [6.5        3.25      ]]\n",
      "数据点所属的簇：\n",
      "[0 0 1 0 0 1 0 1 1 0]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "# 数据集\n",
    "data = np.array([[3, 4], [3, 6], [7, 3], [4, 7], [3, 8], [8, 5], [4, 5], [4, 1], [7, 4], [5, 5]])\n",
    "\n",
    "# 初始簇中心\n",
    "centroids = np.array([[3, 6], [4, 1]])\n",
    "\n",
    "# 迭代计算\n",
    "for iteration in range(3):\n",
    "    # 计算每个数据点到簇中心的距离\n",
    "    distances = np.array([[distance(data[i], centroids[j]) for j in range(len(centroids))] for i in range(len(data))])\n",
    "\n",
    "    # 分配数据点到最近的簇中心\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "\n",
    "    # 更新簇中心\n",
    "    new_centroids = np.array([data[labels == j].mean(axis=0) for j in range(len(centroids))])\n",
    "\n",
    "    # 打印迭代结果\n",
    "    print(f\"第 {iteration+1} 轮迭代结果：\")\n",
    "    print(\"数据点到质心的距离：\")\n",
    "    print(distances)\n",
    "    print(\"簇质心的计算过程：\")\n",
    "    print(new_centroids)\n",
    "    print(\"数据点所属的簇：\")\n",
    "    print(labels)\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "    # 检查是否收敛\n",
    "    if np.all(new_centroids == centroids):\n",
    "        break\n",
    "\n",
    "    centroids = new_centroids\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
